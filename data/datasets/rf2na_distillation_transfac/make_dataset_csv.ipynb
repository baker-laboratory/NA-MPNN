{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF2NA Transfac Distillation Set\n",
    "\n",
    "This notebook can be used to process Lily McHugh's Transfac structure-predicted specificity dataset into a csv that can be utilized for MPNN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv from Lily's distillation set.\n",
    "initial_csv_path = \"/projects/ml/prot_dna/transfac/fasta_v2/rf2_scores.csv\"\n",
    "\n",
    "# Directory containing the predicted PDBs.\n",
    "structure_directory = \"/projects/ml/prot_dna/transfac/fasta_v2\"\n",
    "\n",
    "# Path to factor ID to PCM ID mapping.\n",
    "factor_id_to_ppm_id_csv_path = \"/home/akubaney/projects/data/transfac_2023_05_30/factor_df_seq_mat_best_out.csv\"\n",
    "\n",
    "# Path to the file containing the raw pcms.\n",
    "raw_pcms_path = \"/home/akubaney/projects/data/transfac_2023_05_30/matrix.dat\"\n",
    "\n",
    "# Directories for preprocessed data.\n",
    "preprocessed_ppms_directory = \"./preprocessed_ppms\"\n",
    "preprocessed_ppms_directory = os.path.abspath(preprocessed_ppms_directory)\n",
    "\n",
    "preprocessed_data_directory = \"./preprocessed_data\"\n",
    "preprocessed_data_directory = os.path.abspath(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(path):\n",
    "    with open(path, mode=\"rt\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Initial CSV\n",
    "\n",
    "This notebook starts from the Lily's CSV for the distillation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(initial_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter based on i_pae and plddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.i_pae <= 6) & (df.plddt >= 0.85)].copy()\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the ID, Structure Path, Date, and Dataset Name and Drop Unnecessary Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df[\"tag\"] + \"_pred\"\n",
    "\n",
    "df[\"factor_id\"] = df[\"tag\"].str.slice(0, 6)\n",
    "\n",
    "df[\"structure_path\"] = structure_directory + os.sep + df[\"factor_id\"].str.slice(1, 3) + os.sep + df[\"factor_id\"] + os.sep + df[\"id\"] + \".pdb\"\n",
    "\n",
    "df[\"date\"] = \"1970-01-01\"\n",
    "\n",
    "df[\"dataset_name\"] = \"rf2na_distillation_transfac\"\n",
    "\n",
    "df = df[[\"id\", \"structure_path\", \"date\", \"dataset_name\", \"factor_id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Structure Files into Interface Masks, Base Pair Masks, Sequence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a temporary dataframe, to be used to do preprocessing.\n",
    "df.to_csv(\"./preprocessing_input.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_data_directory):\n",
    "    shutil.rmtree(preprocessed_data_directory)\n",
    "os.makedirs(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following, starting from the directory that this script lives in.\n",
    "\n",
    "```\n",
    "cd /home/akubaney/projects/na_mpnn/data\n",
    "\n",
    "dataset_directory=\"./datasets/rf2na_distillation_transfac\"\n",
    "\n",
    "input_csv_path=$dataset_directory\"/preprocessing_input.csv\"\n",
    "output_directory=$dataset_directory\"/preprocessed_data\"\n",
    "preprocessing_tmp_path=$dataset_directory\"/preprocessing_tmp.out\"\n",
    "\n",
    "rm $preprocessing_tmp_path\n",
    "\n",
    "sbatch --output=$preprocessing_tmp_path --array=0-999 ./preprocess_dataset.sh $input_csv_path $output_directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessing input dataframe.\n",
    "df = pd.read_csv(\"./preprocessing_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove examples that failed preprocessing.\n",
    "failed_directory = os.path.join(preprocessed_data_directory, \"bad\")\n",
    "failed_preprocessing_ids = []\n",
    "reasons_for_failure_count = dict()\n",
    "for file_name in os.listdir(failed_directory):\n",
    "    id = os.path.splitext(file_name)[0]\n",
    "    failed_preprocessing_ids.append(id)\n",
    "\n",
    "    file_path = os.path.join(failed_directory, file_name)\n",
    "    reason_for_failure = read_text_file(file_path)\n",
    "    reasons_for_failure_count[reason_for_failure] = reasons_for_failure_count.get(reason_for_failure, 0) + 1\n",
    "\n",
    "print(failed_preprocessing_ids)\n",
    "print(len(failed_preprocessing_ids))\n",
    "print(reasons_for_failure_count)\n",
    "\n",
    "df = df[np.logical_not(np.isin(df.id, failed_preprocessing_ids))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed file paths\n",
    "preprocessed_attribute_names = os.listdir(preprocessed_data_directory)\n",
    "preprocessed_attribute_names.remove(\"bad\")\n",
    "for attribute_name in preprocessed_attribute_names:\n",
    "    attribute_path_dict = dict()\n",
    "    for id in df.id:\n",
    "        if attribute_name == \"sequences\":\n",
    "            extension = \".csv\"\n",
    "        else:\n",
    "            extension = \".npy\"\n",
    "        attribute_path = os.path.join(preprocessed_data_directory, attribute_name, id + extension)\n",
    "        assert(os.path.exists(attribute_path))\n",
    "        attribute_path_dict[id] = attribute_path\n",
    "    df[attribute_name + \"_path\"] = df.id.map(attribute_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess PPMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ppms(all_pcms_path, ppm_output_directory):\n",
    "    \"\"\"\n",
    "    Given a path to a the file containing all the pcms and an output directory,\n",
    "    preprocesse the pcms into ppms and save them in the output directory.\n",
    "\n",
    "    Arguments:\n",
    "        all_pcms_path (str): the path to the text file containing all the pcms.\n",
    "        ppm_output_directory (str): the directory to save the ppms in.\n",
    "\n",
    "    Side Effects:\n",
    "        Saves the ppms in the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(ppm_output_directory, exist_ok = True)\n",
    "\n",
    "    # Load the text from \n",
    "    all_pcms_text = read_text_file(all_pcms_path)\n",
    "    pcm_entries = all_pcms_text.strip().split(\"//\\n\")[1:]\n",
    "\n",
    "    # Load each pcm, convert into ppm, and save.\n",
    "    for pcm_entry in pcm_entries:\n",
    "        ppm_id = None\n",
    "        pcm_lines = []\n",
    "        is_reading_pcm_lines = False\n",
    "        for line in pcm_entry.strip().split(\"\\n\"):\n",
    "            # Clean up the line formatting.\n",
    "            line = line.strip()\n",
    "            while \"  \" in line:\n",
    "                line = line.replace(\"  \", \" \")\n",
    "\n",
    "            # Record the PPM ID.\n",
    "            if line.startswith(\"AC\"):\n",
    "                ppm_id = line.split(\" \")[1]\n",
    "            # Record the PCM header and start reading pcm lines.\n",
    "            elif line.startswith(\"P0\"):\n",
    "                # Add an extra temporary column to catch the extra column of data\n",
    "                # in the pcm.\n",
    "                if len(line.split(\" \")) == 5:\n",
    "                    line += \" TEMP\"\n",
    "                \n",
    "                pcm_lines.append(line)\n",
    "                is_reading_pcm_lines = True\n",
    "            # Record the PCM line.\n",
    "            elif is_reading_pcm_lines:\n",
    "                if line.startswith(\"XX\"):\n",
    "                    break\n",
    "                else:\n",
    "                    pcm_lines.append(line)\n",
    "\n",
    "        # Create the pcm and remove unnecessary columns.\n",
    "        pcm_df = pd.read_csv(io.StringIO(\"\\n\".join(pcm_lines)), delimiter = \" \")\n",
    "        pcm_df = pcm_df.drop(columns = [\"P0\", \"TEMP\"])\n",
    "        \n",
    "        # Turn the pcm into a ppm.\n",
    "        ppm_df = pcm_df.div(pcm_df.sum(axis = 1), axis = 0)\n",
    "\n",
    "        # Determine the ppm output path.\n",
    "        ppm_output_path = os.path.join(ppm_output_directory, ppm_id + \".csv\")\n",
    "\n",
    "        # Assert that nothing exists at the output path.\n",
    "        assert(not os.path.exists(ppm_output_path))\n",
    "\n",
    "        # Save the ppm.\n",
    "        ppm_df.to_csv(ppm_output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_ppms_directory):\n",
    "    shutil.rmtree(preprocessed_ppms_directory)\n",
    "os.makedirs(preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_ppms(raw_pcms_path, preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match PPMs with PDB IDs\n",
    "\n",
    "Use the TF_Information to Pair Structures and PPMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe containing the factor id and ppm ids.\n",
    "factor_id_to_ppm_id_df = pd.read_csv(factor_id_to_ppm_id_csv_path)\n",
    "\n",
    "# Compute the ppm paths from the matrix ids.\n",
    "factor_id_to_ppm_paths = dict()\n",
    "missing_ppm_ids = []\n",
    "factor_id_with_no_ppm_ids = []\n",
    "for factor_id, ppm_ids_str in zip(factor_id_to_ppm_id_df[\"factor_id\"], factor_id_to_ppm_id_df[\"matrix_ids\"]):\n",
    "    # If the ppm ids are empty, continue.\n",
    "    if pd.isna(ppm_ids_str):\n",
    "        factor_id_with_no_ppm_ids.append(factor_id)\n",
    "        continue\n",
    "\n",
    "    # Split the string representation of the ppm ids.\n",
    "    if ppm_ids_str.startswith(\";\"):\n",
    "        ppm_ids_str = ppm_ids_str[1:]\n",
    "    ppm_ids = ppm_ids_str.split(\";\")\n",
    "\n",
    "    # Turn the ppm ids into ppm paths, and record them if they exist.\n",
    "    ppm_paths = []\n",
    "    for ppm_id in ppm_ids:\n",
    "        ppm_path = os.path.join(preprocessed_ppms_directory, ppm_id + \".csv\")\n",
    "        if os.path.exists(ppm_path):\n",
    "            ppm_paths.append(ppm_path)\n",
    "        else:\n",
    "            missing_ppm_ids.append(ppm_id)\n",
    "    \n",
    "    if len(ppm_paths) > 0:\n",
    "        factor_id_to_ppm_paths[factor_id] = [tuple(ppm_paths)]\n",
    "\n",
    "# print(factor_id_to_ppm_paths)\n",
    "print(len(factor_id_to_ppm_paths))\n",
    "print(missing_ppm_ids)\n",
    "print(len(missing_ppm_ids))\n",
    "print(factor_id_with_no_ppm_ids)\n",
    "print(len(factor_id_with_no_ppm_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ppm_paths\"] = df.factor_id.apply(lambda factor_id: factor_id_to_ppm_paths.get(factor_id, []))\n",
    "\n",
    "# The gene ID column is no longer needed.\n",
    "df = df.drop(columns = [\"factor_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Preprocessing Output Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./preprocessing_output.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
