{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF2NA CisBP Distillation Set\n",
    "\n",
    "This notebook can be used to process Lily McHugh's CisBP structure-predicted specificity dataset into a csv that can be utilized for MPNN training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the csv from Lily's distillation set.\n",
    "initial_csv_path = \"/projects/ml/prot_dna/prot_na_distill.v3.csv\"\n",
    "\n",
    "# Directory containing the predicted PDBs.\n",
    "structure_directory = \"/projects/ml/prot_dna/distill_v2/filtered\"\n",
    "\n",
    "# Path to the Gene ID -> PPM Code mapping.\n",
    "tf_information_path = \"/home/akubaney/projects/data/cisBP_2021_06_23/TF_Information.txt\"\n",
    "\n",
    "# Directory containing the raw ppms.\n",
    "raw_ppm_directory = \"/home/akubaney/projects/data/cisBP_2021_06_23/ppms\"\n",
    "\n",
    "# Directories for preprocessed data.\n",
    "preprocessed_ppms_directory = \"./preprocessed_ppms\"\n",
    "preprocessed_ppms_directory = os.path.abspath(preprocessed_ppms_directory)\n",
    "\n",
    "preprocessed_data_directory = \"./preprocessed_data\"\n",
    "preprocessed_data_directory = os.path.abspath(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(path):\n",
    "    with open(path, mode=\"rt\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Initial CSV\n",
    "\n",
    "This notebook starts from the Lily's CSV for the distillation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(initial_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter based on i_pae and plddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.i_pae <= 6) & (df.plddt >= 0.85)].copy()\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the ID, Structure Path, Date, and Dataset Name and Drop Unnecessary Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df[\"gene_id\"] + \"_\" + df[\"DNA sequence\"]\n",
    "\n",
    "df[\"structure_path\"] = structure_directory + os.sep + df[\"gene_id\"].str.slice(0, 2) + os.sep + df[\"id\"] + \".pdb\"\n",
    "\n",
    "df[\"date\"] = \"1970-01-01\"\n",
    "\n",
    "df[\"dataset_name\"] = \"rf2na_distillation_cis_bp\"\n",
    "\n",
    "df = df[[\"id\", \"structure_path\", \"date\", \"dataset_name\", \"gene_id\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Structure Files into Interface Masks, Base Pair Masks, Sequence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a temporary dataframe, to be used to do preprocessing.\n",
    "df.to_csv(\"./preprocessing_input.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_data_directory):\n",
    "    shutil.rmtree(preprocessed_data_directory)\n",
    "os.makedirs(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following, starting from the directory that this script lives in.\n",
    "\n",
    "```\n",
    "cd /home/akubaney/projects/na_mpnn/data\n",
    "\n",
    "dataset_directory=\"./datasets/rf2na_distillation_cis_bp\"\n",
    "\n",
    "input_csv_path=$dataset_directory\"/preprocessing_input.csv\"\n",
    "output_directory=$dataset_directory\"/preprocessed_data\"\n",
    "preprocessing_tmp_path=$dataset_directory\"/preprocessing_tmp.out\"\n",
    "\n",
    "rm $preprocessing_tmp_path\n",
    "\n",
    "sbatch --output=$preprocessing_tmp_path --array=0-499 ./preprocess_dataset.sh $input_csv_path $output_directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessing input dataframe.\n",
    "df = pd.read_csv(\"./preprocessing_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove examples that failed preprocessing.\n",
    "failed_directory = os.path.join(preprocessed_data_directory, \"bad\")\n",
    "failed_preprocessing_ids = []\n",
    "reasons_for_failure_count = dict()\n",
    "for file_name in os.listdir(failed_directory):\n",
    "    id = os.path.splitext(file_name)[0]\n",
    "    failed_preprocessing_ids.append(id)\n",
    "\n",
    "    file_path = os.path.join(failed_directory, file_name)\n",
    "    reason_for_failure = read_text_file(file_path)\n",
    "    reasons_for_failure_count[reason_for_failure] = reasons_for_failure_count.get(reason_for_failure, 0) + 1\n",
    "\n",
    "print(failed_preprocessing_ids)\n",
    "print(len(failed_preprocessing_ids))\n",
    "print(reasons_for_failure_count)\n",
    "\n",
    "df = df[np.logical_not(np.isin(df.id, failed_preprocessing_ids))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed file paths\n",
    "preprocessed_attribute_names = os.listdir(preprocessed_data_directory)\n",
    "preprocessed_attribute_names.remove(\"bad\")\n",
    "for attribute_name in preprocessed_attribute_names:\n",
    "    attribute_path_dict = dict()\n",
    "    for id in df.id:\n",
    "        if attribute_name == \"sequences\":\n",
    "            extension = \".csv\"\n",
    "        else:\n",
    "            extension = \".npy\"\n",
    "        attribute_path = os.path.join(preprocessed_data_directory, attribute_name, id + extension)\n",
    "        assert(os.path.exists(attribute_path))\n",
    "        attribute_path_dict[id] = attribute_path\n",
    "    df[attribute_name + \"_path\"] = df.id.map(attribute_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess PPMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_ppm(raw_ppm_path):\n",
    "    \"\"\"\n",
    "    Given a path to a raw ppm, return an Lx4 numpy array of the ppm.\n",
    "\n",
    "    Arguments:\n",
    "        raw_ppm_path (str): the path to the raw ppm.\n",
    "    \n",
    "    Returns:\n",
    "        ppm_df (np.float64 np.ndarray): an Lx4 dataframe of the ppm, where the \n",
    "            columns are A, C, G, T.\n",
    "    \"\"\"\n",
    "    ppm_text = read_text_file(raw_ppm_path)\n",
    "\n",
    "    # Remove unnecessary text at the top.\n",
    "    ppm_array_text = \"position\" + ppm_text.split(\"Pos\")[-1]\n",
    "\n",
    "    # Read the ppm as a csv.\n",
    "    ppm_df = pd.read_csv(io.StringIO(ppm_array_text), sep = \"\\t\")\n",
    "\n",
    "    # Get rid of the position column.\n",
    "    ppm_df = ppm_df.drop(columns = [\"position\"])\n",
    "\n",
    "    return ppm_df\n",
    "\n",
    "def preprocess_ppms(raw_ppm_directory, ppm_output_directory):\n",
    "    os.makedirs(ppm_output_directory, exist_ok = True)\n",
    "    \n",
    "    for raw_ppm_file_name in os.listdir(raw_ppm_directory):\n",
    "        raw_ppm_path = os.path.join(raw_ppm_directory, raw_ppm_file_name)\n",
    "\n",
    "        # Remove the extension from the file name. Note, this allows for file\n",
    "        # names that have '.' in the name.\n",
    "        ppm_name = os.path.splitext(raw_ppm_file_name)[0]\n",
    "\n",
    "        # Load the ppm.\n",
    "        ppm_df = load_raw_ppm(raw_ppm_path)\n",
    "\n",
    "        # Save the ppm.\n",
    "        ppm_output_path = os.path.join(ppm_output_directory, ppm_name + \".csv\")\n",
    "\n",
    "        # Assert that nothing exists at the output path.\n",
    "        assert(not os.path.exists(ppm_output_path))\n",
    "\n",
    "        # Save the ppm.\n",
    "        ppm_df.to_csv(ppm_output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_ppms_directory):\n",
    "    shutil.rmtree(preprocessed_ppms_directory)\n",
    "os.makedirs(preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_ppms(raw_ppm_directory, preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match PPMs with PDB IDs\n",
    "\n",
    "Use the TF_Information to Pair Structures and PPMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the information from the TF_Information.txt file.\n",
    "tf_information_str = read_text_file(tf_information_path)\n",
    "\n",
    "# Make some formatting corrections.\n",
    "tf_information_str = tf_information_str.replace(\",\\t\", \",\").replace(\",\\n\", \"\\n\")\n",
    "\n",
    "# Load the information as a dataframe.\n",
    "tf_information_df = pd.read_csv(io.StringIO(tf_information_str), sep = \"\\t\")\n",
    "\n",
    "# Comptue the ppm paths from the motif ids.\n",
    "gene_id_to_ppm_paths = dict()\n",
    "missing_ppm_ids = []\n",
    "for gene_id, ppm_ids_str in zip(tf_information_df[\"Gene_ID\"], tf_information_df[\"Motif_ID\"]):\n",
    "    ppm_ids = ppm_ids_str.split(\",\")\n",
    "\n",
    "    # Turn the ppm ids into ppm paths, and record them if they exist.\n",
    "    ppm_paths = []\n",
    "    for ppm_id in ppm_ids:\n",
    "        ppm_path = os.path.join(preprocessed_ppms_directory, ppm_id + \".csv\")\n",
    "        if os.path.exists(ppm_path):\n",
    "            ppm_paths.append(ppm_path)\n",
    "        else:\n",
    "            missing_ppm_ids.append(ppm_id)\n",
    "    \n",
    "    if len(ppm_paths) > 0:\n",
    "        gene_id_to_ppm_paths[gene_id] = [tuple(ppm_paths)]\n",
    "\n",
    "print(gene_id_to_ppm_paths)\n",
    "print(len(gene_id_to_ppm_paths))\n",
    "print(missing_ppm_ids)\n",
    "print(len(missing_ppm_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ppm_paths\"] = df.gene_id.apply(lambda gene_id: gene_id_to_ppm_paths.get(gene_id, []))\n",
    "\n",
    "# The gene ID column is no longer needed.\n",
    "df = df.drop(columns = [\"gene_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Preprocessing Output Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./preprocessing_output.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
