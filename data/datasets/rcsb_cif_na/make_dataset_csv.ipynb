{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCSB CIF Nucleic Acid Dataset\n",
    "\n",
    "Andrew Kubaney (akubaney)\n",
    "\n",
    "This notebook can be used to process the nucleic acid-containing entries in the RCSB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import io\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the preprocessed csv from the RCSB CIF dataset.\n",
    "initial_csv_path = \"/home/akubaney/projects/na_mpnn/data/datasets/rcsb_cif/pdb_21Jan2025.csv\"\n",
    "\n",
    "# Directory containing the CIFs.\n",
    "structure_directory = \"/databases/rcsb/cif\"\n",
    "\n",
    "# Path to DeepPBS PDB+chain ID to PCM ID mapping.\n",
    "pdb_chain_id_and_pcm_id_path = \"./jaspar_h11mo_cluster_wise_dna_containing_dataset.npy\"\n",
    "\n",
    "# Directories containing the raw PCMs.\n",
    "raw_pcm_directory_jaspar = \"/home/akubaney/projects/data/jaspar_2025_02_05/pcms\"\n",
    "raw_pcm_directory_hocomoco_v11_human = \"/home/akubaney/projects/data/hocomoco_v11_2025_02_05/human/pcms\"\n",
    "raw_pcm_directory_hocomoco_v11_mouse = \"/home/akubaney/projects/data/hocomoco_v11_2025_02_05/mouse/pcms\"\n",
    "\n",
    "# Directories for preprocessed data.\n",
    "preprocessed_data_directory = \"./preprocessed_data\"\n",
    "preprocessed_data_directory = os.path.abspath(preprocessed_data_directory)\n",
    "\n",
    "preprocessed_ppms_directory = \"./preprocessed_ppms\"\n",
    "preprocessed_ppms_directory = os.path.abspath(preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(path):\n",
    "    with open(path, mode=\"rt\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Initial CSV\n",
    "\n",
    "This notebook starts from the RCSB CIF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(initial_csv_path)\n",
    "\n",
    "# Turn strings into lists.\n",
    "tolist = lambda l : l[1:-1].replace(\"'\",\"\").split(\", \")\n",
    "for key in ('poly','poly_type','nonpoly','poly_sequence'):\n",
    "    df[key] = df[key].apply(tolist)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter based on Number of Heavy Atoms, Coverage, Number of Unknown Residues, Resolution, and Presence of Nucleic Acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on number of heavy atoms and coverage.\n",
    "sel = ((df.num_heavy>=100) & (df.coverage>=0.9))\n",
    "\n",
    "# Filter sequences with too many unknown residues.\n",
    "def seq_filter(seqs):\n",
    "\n",
    "    maxX = 20\n",
    "    \n",
    "    Lmax = 0 if len(seqs)<1 else max([len(s) for s in seqs])\n",
    "    s = \"\".join(seqs)\n",
    "    L = len(s)\n",
    "    if Lmax<=maxX:\n",
    "        return True\n",
    "\n",
    "    top_aa = collections.Counter(s).most_common(1)[0]\n",
    "    if top_aa[0]=='X' and top_aa[1]>maxX:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "sel = sel & (df.poly_sequence.apply(seq_filter))\n",
    "\n",
    "# Filter sequences based on resolution.\n",
    "# In this case, include nan resolution, since this captures NMR structures.\n",
    "sel = sel & ((df.resolution<=3.5) | (np.isnan(df.resolution)))\n",
    "\n",
    "# Filter entries with no nucleic acid.\n",
    "def chains_contain_nucleic_acid(chain_types):\n",
    "    return \"polydeoxyribonucleotide/polyribonucleotide hybrid\" in chain_types or \\\n",
    "           \"polydeoxyribonucleotide\" in chain_types or \\\n",
    "           \"polyribonucleotide\" in chain_types\n",
    "\n",
    "sel = sel & (df.poly_type.apply(chains_contain_nucleic_acid))\n",
    "\n",
    "df = df[sel].copy()\n",
    "\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Structure Path and Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df[\"label\"]\n",
    "\n",
    "df[\"structure_path\"] = structure_directory + os.sep + df[\"id\"].str.slice(1,3) + os.sep + df[\"id\"] + \".cif.gz\"\n",
    "\n",
    "df[\"dataset_name\"] = \"rcsb_cif_na\"\n",
    "\n",
    "df = df[[\"id\", \"structure_path\", \"date\", \"dataset_name\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Structure Files into Interface Masks, Base Pair Masks, Sequence, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a temporary dataframe, to be used to do preprocessing.\n",
    "df.to_csv(\"./preprocessing_input.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_data_directory):\n",
    "    shutil.rmtree(preprocessed_data_directory)\n",
    "os.makedirs(preprocessed_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following, starting from the directory that this script lives in.\n",
    "\n",
    "```\n",
    "cd /home/akubaney/projects/na_mpnn/data\n",
    "\n",
    "dataset_directory=\"./datasets/rcsb_cif_na\"\n",
    "\n",
    "input_csv_path=$dataset_directory\"/preprocessing_input.csv\"\n",
    "output_directory=$dataset_directory\"/preprocessed_data\"\n",
    "preprocessing_tmp_path=$dataset_directory\"/preprocessing_tmp.out\"\n",
    "\n",
    "rm $preprocessing_tmp_path\n",
    "\n",
    "sbatch --output=$preprocessing_tmp_path --array=0-499 ./preprocess_dataset.sh $input_csv_path $output_directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessing input dataframe.\n",
    "df = pd.read_csv(\"./preprocessing_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove examples that failed preprocessing.\n",
    "failed_directory = os.path.join(preprocessed_data_directory, \"bad\")\n",
    "failed_preprocessing_ids = []\n",
    "reasons_for_failure_count = dict()\n",
    "for file_name in os.listdir(failed_directory):\n",
    "    id = os.path.splitext(file_name)[0]\n",
    "    failed_preprocessing_ids.append(id)\n",
    "\n",
    "    file_path = os.path.join(failed_directory, file_name)\n",
    "    reason_for_failure = read_text_file(file_path)\n",
    "    reasons_for_failure_count[reason_for_failure] = reasons_for_failure_count.get(reason_for_failure, 0) + 1\n",
    "\n",
    "print(failed_preprocessing_ids)\n",
    "print(len(failed_preprocessing_ids))\n",
    "print(reasons_for_failure_count)\n",
    "\n",
    "df = df[np.logical_not(np.isin(df.id, failed_preprocessing_ids))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed file paths\n",
    "preprocessed_attribute_names = os.listdir(preprocessed_data_directory)\n",
    "preprocessed_attribute_names.remove(\"bad\")\n",
    "for attribute_name in preprocessed_attribute_names:\n",
    "    attribute_path_dict = dict()\n",
    "    for id in df.id:\n",
    "        if attribute_name == \"sequences\":\n",
    "            extension = \".csv\"\n",
    "        else:\n",
    "            extension = \".npy\"\n",
    "        attribute_path = os.path.join(preprocessed_data_directory, attribute_name, id + extension)\n",
    "        assert(os.path.exists(attribute_path))\n",
    "        attribute_path_dict[id] = attribute_path\n",
    "    df[attribute_name + \"_path\"] = df.id.map(attribute_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess PCMs into PPMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ppm_from_raw_pcm(raw_pcm_path, pcm_format):\n",
    "    \"\"\"\n",
    "    Given a path to a raw pcm, return an Lx4 numpy array of the ppm.\n",
    "\n",
    "    Arguments:\n",
    "        raw_pcm_path (str): the path to the raw pcm.\n",
    "    \n",
    "    Returns:\n",
    "        ppm_df (np.float64 np.ndarray): an Lx4 dataframe of the ppm, where the \n",
    "            columns are A, C, G, T.\n",
    "    \"\"\"\n",
    "    pcm_text = read_text_file(raw_pcm_path)\n",
    "    pcm_text = pcm_text.strip()\n",
    "\n",
    "    # Jaspar format.\n",
    "    if pcm_format == \"jaspar\":\n",
    "        # Exclude the header.\n",
    "        pcm_lines = pcm_text.split(\"\\n\")[1:]\n",
    "\n",
    "        # Dictionary to create the dataframe.\n",
    "        data_dict = dict()\n",
    "\n",
    "        # Extract the counts for each base.\n",
    "        for line in pcm_lines:\n",
    "            # Standardize the line.\n",
    "            line = line.strip()\n",
    "            line = line.replace(\" ]\", \"\")\n",
    "            while \"  \" in line:\n",
    "                line = line.replace(\"  \", \" \")\n",
    "            \n",
    "            # Extract the base name and the counts array text.\n",
    "            base, base_counts_str = line.split(\" [\")\n",
    "            base_counts_str = base_counts_str.strip()\n",
    "            base_counts = list(map(lambda count_str: int(count_str), base_counts_str.split(\" \")))\n",
    "\n",
    "            data_dict[base] = base_counts\n",
    "        \n",
    "        # Create the pcm dataframe.\n",
    "        pcm_df = pd.DataFrame(data_dict)\n",
    "    # Hocomoco format.\n",
    "    elif pcm_format == \"hocomoco\":\n",
    "        # Exclude the header.\n",
    "        pcm_lines = pcm_text.split(\"\\n\")[1:]\n",
    "\n",
    "        # Read the pcm dataframe.\n",
    "        pcm_df = pd.read_csv(io.StringIO(\"\\n\".join(pcm_lines)), sep = \"\\t\", names = [\"A\", \"C\", \"G\", \"T\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pcm_format: {pcm_format}\")\n",
    "        \n",
    "    # Turn the pcm into a ppm.\n",
    "    ppm_df = pcm_df.div(pcm_df.sum(axis = 1), axis = 0)\n",
    "\n",
    "    return ppm_df\n",
    "\n",
    "def preprocess_pcms_into_ppms(raw_pcm_directory, ppm_output_directory, pcm_format):\n",
    "    os.makedirs(ppm_output_directory, exist_ok = True)\n",
    "\n",
    "    for raw_pcm_file_name in os.listdir(raw_pcm_directory):\n",
    "        raw_pcm_path = os.path.join(raw_pcm_directory, raw_pcm_file_name)\n",
    "\n",
    "        # Remove the extension from the file name. Note, this allows for file\n",
    "        # names that have '.' in the name.\n",
    "        if raw_pcm_file_name.endswith(\".jaspar\"):\n",
    "            ppm_name = raw_pcm_file_name\n",
    "        else:\n",
    "            ppm_name = os.path.splitext(raw_pcm_file_name)[0]\n",
    "\n",
    "        # Load the raw pcm and preprocess it into a ppm.\n",
    "        ppm_df = load_ppm_from_raw_pcm(raw_pcm_path, pcm_format)\n",
    "\n",
    "        # Determine the ppm output path.\n",
    "        ppm_output_path = os.path.join(ppm_output_directory, ppm_name + \".csv\")\n",
    "\n",
    "        # Assert that nothing exists at the output path.\n",
    "        assert(not os.path.exists(ppm_output_path))\n",
    "\n",
    "        # Save the ppm.\n",
    "        ppm_df.to_csv(ppm_output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(preprocessed_ppms_directory):\n",
    "    shutil.rmtree(preprocessed_ppms_directory)\n",
    "os.makedirs(preprocessed_ppms_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pcms_into_ppms(raw_pcm_directory_jaspar, preprocessed_ppms_directory, \"jaspar\")\n",
    "preprocess_pcms_into_ppms(raw_pcm_directory_hocomoco_v11_human, preprocessed_ppms_directory, \"hocomoco\")\n",
    "preprocess_pcms_into_ppms(raw_pcm_directory_hocomoco_v11_mouse, preprocessed_ppms_directory, \"hocomoco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match PPMs with PDB IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDB+chain ID to PCM label from DeepPBS. This data comes \n",
    "# pre-clustered; it will be re-clustered later.\n",
    "pdb_chain_id_and_pcm_id = np.load(pdb_chain_id_and_pcm_id_path, allow_pickle = True)\n",
    "\n",
    "pdb_id_to_ppm_paths = dict()\n",
    "missing_ppm_ids = []\n",
    "for cluster_list in pdb_chain_id_and_pcm_id:\n",
    "    for (pdb_chain_id, chain_pcm_ids) in cluster_list:\n",
    "        pdb_id, chain_id = pdb_chain_id.split(\"_\")\n",
    "        \n",
    "        ppm_paths = []\n",
    "        for pcm_id in chain_pcm_ids:\n",
    "            ppm_path = os.path.join(preprocessed_ppms_directory, pcm_id + \".csv\")\n",
    "            if os.path.exists(ppm_path):\n",
    "                ppm_paths.append(ppm_path)\n",
    "            else:\n",
    "                missing_ppm_ids.append(pcm_id)\n",
    "        \n",
    "        if len(ppm_paths) > 0:\n",
    "            if pdb_id not in pdb_id_to_ppm_paths:\n",
    "                pdb_id_to_ppm_paths[pdb_id] = []\n",
    "            \n",
    "            pdb_id_to_ppm_paths[pdb_id].append(tuple(ppm_paths))\n",
    "\n",
    "print(pdb_id_to_ppm_paths)\n",
    "print(len(pdb_id_to_ppm_paths))\n",
    "print(missing_ppm_ids)\n",
    "print(len(missing_ppm_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ppm_paths\"] = df.id.apply(lambda id: pdb_id_to_ppm_paths.get(id, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Preprocessing Output Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./preprocessing_output.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
